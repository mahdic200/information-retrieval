# E07

## شاخص tf-idf weighting

#### $$W\ _{t,d} \ = \ 1 + \log_{10}(\ tf\ _{t,d} \ ) \times \log_{10}(N \div df_t)$$ 
در این شاخص دو عدد در هم ضرب میشوند ، tf و idf . برای هر واژه ی موجود در کوئری اگر اون واژه در یک سند خاصی زیاد تکرار شده باشه و اون واژه واژه ایی نباشه که تو سند های زیادی تکرار شده باشه آنوقت رتبه ی اون سند برای جست و جو باید بالاتر بره . ما برای تک تک واژه های موجود در کوئری این رو باید اندازه گیری بکنیم و باهم جمع کنیم .

این رابطه اییه که برای امتیاز دادن به هر سند استفاده میشه . و یعنی امتیازی که ما به سند d میدهیم برای کوئری q .
ما به تک تک سند های موجود در پایگاه داده ی خود امتیاز میدیم و سپس اونهارو برای نتایج جست و جو بر اساس همون امتیاز ها رتبه بندی میکنیم .

این تیکه $t\ \in \ (q \ \cap \ d)$ یعنی برای واژه هایی از کوئری که در سند d وجود دارند (t به معنی واژه است) . یعنی واژه هایی که در کوئری نیستند رو ما اصلا حساب نمیکنیم و برامون مهم نیست .

#### $$Score(q,d) = \sum_{t \ \in \ (q \ \cap \ d)}{tf.idf_{t,d}}$$
در اینجا idf داره به گونه ایی ضریب میده به tf . و اگر پر تکرار باشه .
سندی امتیاز زیادی میگیرد که واژه های کلیدی متن جست و جو در آن زیاد تکرار شده باشد (نه چیز هایی مث در ، از ، با) .

## فضای برداری

ما هر سند رو میتوانیم به صورت یک بردار در نظر بگیریم . و این یعنی توی فضای برداری یک سری محور داریم که در این سند ها واژه های واژه نامه میشوند محور های ما .

آقا خلاصه ی مطلب ، ی ماتریس میسازه مث فصل یک ، منتها به جای تعداد تکرار توی درایه هاش نتایج مقیاس بالا رو میزاره .

حالا که مسئله عددی شد ، میتونیم از خواص بردار ها استفاده کنیم و سند های شبیه به هم رو در یک خوشه قرار بدیم که برای این کار به یک معیاری نیاز داریم که الان بهش میپردازیم .

## معیار شباهت سند های وزن دهی شده با tf-idf

شباهت عکس فاصله است . چه فاصله ایی ؟ فاصله ی اقلیدسی به این صورت هست برای یک بردار چند بعدی یا ماتریس (برای دو بعد) ، فرض کن دو تا ماتریس A و B داریم :

$$result = \sqrt{A^2 + B^2}$$

> [!IMPORTANT]
> این ایده ی بدیه ، چرا ؟ چون خیلی به طول متن درون سند وابسته است .

فرض کنید یک متنی رو جست و جو کردیم و اون بردار tf-idf خودش رو داره . حالا یک سندی داریم که یک فاصله ی اقلیدسی به مقدار d (یک عدد ثابت مانند ۱ و ۱۵و ۵۰ و ۳۹) از بردار متن جست و جوی ما داره . حالا اگر سند مورد نظر رو برداریم و محتواش رو درون خودش و در انتهای خودش کپی کنیم ، چه اتفاقی میفته ؟ فاصله شدیدا افزایش پیدا میکنه ، و این در هنگامیه که درون اون سند از نظر معنایی هیچ تغییری نکرده .

ممکنه ما یک سند بسیار طولانی داشته باشیم و چند تا واژه از متن جست و جو هم دارا باشه ، فاصله ی اقلیدسی نمیتونه این رو تشخیص بده .

چه چیزی میتونه معیار خوبی باشه ؟ زاویه ی میان بردار متن جست و جو و سند . هرچه بیشتر باشه دورتره و هرچه کمتر باشه نزدیک تره .

حال چجوری این زاویه رو بدست بیاریم ؟ کسینوس بین این دو بردار رو حساب میکنیم .

#### $$\cos\left (\vec{q} \ , \vec{d} \ \right) = { {\vec{q} \ \bullet \ \vec{d} }\over{ \left |\vec{q} \right | \left |\vec{d} \right |} } = { { {\vec{q}}\over{\left | \vec{q} \right |} } \bullet { {\vec{d}} \over {\left |\vec{d} \right |} } } = {{ \sum^{\left | V \right | }_{i=1} q_id_i } \over { \sqrt{\sum^{\left | V \right | }_{i=1} {q_i}^2 } \sqrt{\sum^{\left | V \right | }_{i=1} {d_i}^2 } }} $$
توجه کنید که وقتی بردار هارو نرمالیزه کنید فرمول به این صورت تغییر میکنه :

#### $$\cos\left (\vec{q} \ , \vec{d} \ \right) = {\vec{q} \ \bullet \ \vec{d} }  = \sum^{\left | V \right |}_{i=1} q_id_i $$

## چرا به جای فرمول ltc ما از lnc برای سند ها استفاده میکنیم ؟

پاسخ بسیار بدیهی هست ، اصلا هدف idf چی بود ؟ میزان منحصر به فرد بودن (درواقع نوعی شاخص) واژه های متن جست و جو هدف بود . یعنی از واژه های توی متن جست و جو ببینیم ارزش کدوم بیشتره ، کدوم کم یاب تره ، و توی کدوم سند وجود داره ، و این اصلا برای خود سند معنی نمیده ، چون ***اصلا مهم نیست که واژه های توی سند یکتا باشند یا خیر*** .

## برای وزن دهی document frequency از idf استفاده نمیکنند

چرا ؟ دلیلش پاسخ بالا است .

ترجیح میدم تمام چیز هایی که از بخش دوم قسمت هفت مونده رو ارجاع بدم به ویدیوش ، واقعا نمیشه نوشت اونا رو .