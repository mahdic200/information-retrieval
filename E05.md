# E05

# تصحیح غلط های املایی

# مفهوم Edit Distance

برای تصحیح غلط های املایی ، ما باید بگردیم ببینیم واژه ایی که غلطه به کدوم واژه شبیه تره ، و شباهت مفهوم متضاد فاصله است و برای این منظور ما باید به نوعی فاصله رو اندازه گیری کنیم که در ادامه توضیح داده خواهد شد .

## دو اصل معروف در این زمینه

دو تا اصل داریم که یکیش رو استفاده نمیکنن :

- اصل اول این هست که ما سند هایی که قرار است شماره گذاری شوند رو اصلاح بکنیم
- اصل دوم این هست که متن جست و جوی کاربر رو اصلاح بکنیم

> [!IMPORTANT] نکته
> که اصل اول رو استفاده نمیکنن ، چون فرض میکنن فرد نویسنده کارش درسته و سند مشکلی نداره
> چرا ؟ چون ممکنه واژه ایی که شما نادرست تشخیص میدی واژه ایی خاص باشه ، بنا براین ما حق دستکاری سند هارو نداریم .

## دو روش برای گرفتن غلط املایی

دو روش :

- روش Isolated word
- روش Context-sensitive

## روش Isolated word

در این روش تنها با نگاه کردن و توجه کردن به واژه ی مورد نظر بررسی میکنیم که غلط املایی دارد یا خیر . و به واژه ی دیگه ایی نگاه نمیکنیم .
این روش برای مواقعی است که واژه ی وارد شده در هیچ کجا از دیکشنری یا واژه نامه یافت نمیشود و ذاتا شکل نادرستی از یک واژه است . مانند : Coler

نقطه ی ضعف این روش این است که اگر شکل نادرست یا به اشتباه وارد شده ی واژه ی مورد نظر معنی دیگری در واژه نامه بدهد دیگر نمیتوان از این روش استفاده کرد . نمونه : An asteroid that fall form the sky که در اینجا جمله ی ما غلط ساختاری ندارد اما غلط معنایی دارد که باید به جای واژه ی form واژه ی from به کار رود که معنی آن ها به ترتیب `شکل` و `از` میشود .
## روش Contxt-sensitive

این روش حساس به محتوا است و واژه های کنار واژه ی مورد نظر ما هم دقت میکند .


## اصلاح سند ها

فقط در یک صورت ما حق داریم سند های خود را اصلاح کنیم . آن هم در صورتی که سند نیاز به OCR داشته باشد . یعنی اینکه یک عکس داریم و داخل این عکس یک سری واژه ها داریم و با OCR که یک شاخه از پردازش تصویر هست میاییم و سند ها را اسکن میکنیم و استخراج میکنیم و خود OCR خودش خطا دارد و میتوانیم روی نتیجه ی آن اصلاح انجام دهیم . اما نمیتوانیم روی فایل ورد یا پی دی اف پردازش تصویر انجام دهیم چون آن سند مستقیما از یک نویسنده ایی به ما رسیده و ما فرض کردیم که سند ها خطا ندارند .

## اصلاح متن جست و جو یا query

چند شرط وجود دارد :
- نخست آن که یک فهرست از واژگان درست داشته باشیم که اصلاح واژگان نادرست بر پایه ی اینها باشد .
- و دو اینکه ما باید معیاری داشته باشیم که فاصله ی بین دو واژه را اندازه گیری کنیم . فاصله ی بین دو واژه یعنی عدم شباهت بین آن دو .

با شبیه ترین واژه باید واژه ی نادرست را جایگزین کنیم . الگوریتم هایی هستند که این فاصله را اندازه گیری میکنند . نمونه :

```
informaton --> information
```

شرط نخست مشکل ساز است ، چون شما باید یک واژه نامه داشته باشید که دارای همه ی واژه های دنیا باشد و این ناممکن است .

## فاصله ی میان واژه ی درست و نادرست چگونه اندازه گیری میشه ؟

با چند مثال توضیح میدهیم .

## فاصله ی Levenshtein

فاصله ی ویرایشی رو اندازه گیری میکند .

قوانین :

- ویرایش فاصله میان رشته ی یک و رشته ی دو کمینه ی تعداد عملیات پایه است که رشته ی یک را به رشته ی دو تبدیل میکند .
- فاصله ی لون اشتاین : عملیات پایه عبارت اند از insert, delete, replace
- فاصله ی لون اشتاین dog --> do یک واحد است
- فاصله ی cat --> cart یک واحد است
- فاصله ی cat --> cut یک واحد است
- فاصله ی cat --> act یک واحد است

یک نسخه ایی از لون اشتاین هست به نام Damerau-Levenshtein که میگوید ترانسپوز زیر واژه ها فاصله اش یک میشود . مانند cat --> act .
نسخه ی دامورا لون اشتاین transposition رو به عنوان یک عمل چهارم مجاز میداند .

## محاسبه ی فاصله ی لون اشتاین

برای نمونه واژه های cats و fast . مانند زیر واژه ها را مینویسند . سطر اول و ستون اول (در حقیقت سطر اول و ستون اول در این جدول برچسب حساب میشوند و ما با ماتریس درون این جدول که از اعداد پر خواهد شد کار داریم) رو از صفر شروع میکنند .


|     |     | f   | a   | s   | t   |
| --- | --- | --- | --- | --- | --- |
|     | 0   | 1   | 2   | 3   | 4   |
| c   | 1   |     |     |     |     |
| a   | 2   |     |     |     |     |
| t   | 3   |     |     |     |     |
| s   | 4   |     |     |     |     |

 سطر دوم و ستون دوم هم از یک شروع میشوند و تا آخر شماره گذاری میشوند مانند جدول زیر .


|     |     | f   | a   | s   | t   |
| --- | --- | --- | --- | --- | --- |
|     | 0   | 1   | 2   | 3   | 4   |
| c   | 1   | 1   | 2   | 3   | 4   |
| a   | 2   | 2   |     |     |     |
| t   | 3   | 3   |     |     |     |
| s   | 4   | 4   |     |     |     |

و مابقی درایه ها باید بر اساس یک الگوریتمی محاسبه شوند که در اینجا نتیجه سه هست .


|     |     | f   | a   | s   | t   |
| --- | --- | --- | --- | --- | --- |
|     | 0   | 1   | 2   | 3   | 4   |
| c   | 1   | 1   | 2   | 3   | 4   |
| a   | 2   | 2   | 1   | 2   | 3   |
| t   | 3   | 3   | 2   | 2   | 2   |
| s   | 4   | 4   | 3   | 2   | 3   |

## الگوریتم لون اشتاین

```
LevenshteinDistance(s1, s2):
# filling first row
for i <-- 0 to |s1|
do m[i, 0] == i

# filling first column
for j <-- 0 to |s2|
do m[0, j] == j

for i <-- 1 to |s1|
do for j <-- 1 to |s2|
	do if s1[i] == s2[j]
			# check if two characters are equal
			then m[i, j] == min{ m[i-1, j], m[i, j-1]+1, m[i-1, j-1] }
			# else if they are not equal
			else m[i, j] == min{ m[i-1, j]+1, m[i, j-1]+1, m[i-1, j-1]+1 }
return m[|s1|, |s2|]
// Operations: insert (cost 1), delete (cost 1), replace (cost 1), copy (cost 0)
```

## فاصله ی ویرایش وزن دار

احمتال اینکه به جای m شما n رو فشار بدی ، خیلی بیشتر از اینه که بجاش r رو فشار بدی . برای محاسبه ی جایگزینی اومدن و وزن تعریف کردند .

فرض کنید که یک واژه ایی رو اشتباه تایپ کردیم و قراره این اصلاح بشه ، ما گفتیم میایم و فاصله ی این واژه رو با تمام فاصله های توی واژه نامه مقایسه میکنیم و هرکدوم کمتر بود رو پیشنهاد میدیم .

اما نکته ایی که وجود داره اگر چندین کلمه وجود داره که فاصله ی اونها با واژه ی شما یکی بود ، اگر گره پیش بیاد چگونه این گره رو باز میکنیم . ساده ترین راه اینه که واژه ی پرتکرار رو پیشنهاد بدیم ، از میان واژه هایی که فاصله ی یکسانی دارند .

اگر واژه ی اشتباه خودمون رو تک تک با واژه های توی واژه نامه تطبیق بدیم و مقایسه کنیم اونوقت خیلی زمان بر خواهد شد . راه حلش یک سری هیوریستیک پیشنهاد میدند .

یکیش اینه که بیایم و تمام واژه هایی که حرف اولشون با حرف اول واژه ی misspelled یا نادرست در نظر بگیریم . همین حجم محاسبات رو بسیار بسیار کاهش میده .

> [!IMPORTANT] چرا ؟
> چون احتمال اینکه کاربر حرف نخست واژه ی مورد نظرش رو اشتباه وارد کنه بسیار بسیار کمه .

بنا براین یک زیرمجموعه از واژه های واژه نامه رو دخالت میدهند .

### رویکرد دیگر

در این رویکرد سند ها مورد توجه نیستند و تمام توجه به میزان تکرار واژه ها در متن های جست و جو شده ی کاربران میرود .
یعنی ببین کاربرها چی رو بیشتر جست و جو کردند .

# مفهوم Spelling correction

## شاخص k-gram indexes برای Spelling correction

برای اینکه ما بتونیم واژه های کاندیدا برای اصلاح رو پیشنهاد بدیم یک راه اینه که از k-gram استفاده کنیم .
فرض کنید دو واژه دارید و این دو واژه خودشون دو حرفی هایی دارند و اگر اشتراک اینهارو تقسیم کنیم بر اجتماعشون ، نسبت بدست اومده هرچقدر نزدیک به یک باشه یعنی همپوشانی اینها بیشتره .

## مفهوم Context-sensitive spelling correction

راجع به این مفهوم بالاتر توضیح مختصری داده شد .
گفتیم که واژه ی form به خودی خود نادرست نیست اما در جای اشتباه غلط املایی محسوب میشود .
یک راه رو اینجوری معرفی میکنند ، میگن مثلا فرم رو به شکل های گوناگون و با تمام واژه های مشابه جایگذاری کنه و تکرار کل عبارت جدید بدست اومده رو بررسی کنه .
نه فقط برای اون واژه بلکه برای همه ی واژه ها این کاررو میکنیم و میبینیم از بین متن های بدست اومده کدوم از همه بیشتر تکرار شدن .

> [!IMPORTANT]
> مشکل این این روش اینه که در صورت وجود تعداد زیادی از واژه های مشابه شما جایگشت های بسیار زیادی خواهید داشت و دراین صورت بار محاسبات بسیار بسیار زیاد میشه .

## مشکلات عمومی

آیا تصحیح واژگان باید خودکار سازی شود یا اینکه پیشنهاد شود به کاربر .

هزینه ی بسیار زیادی برای این کار وجود دارد . و این کار یعنی اصلاح غلط های املایی رو برای متن هایی انجام میدن که نتایج کمی براش وجود داره و اونوقت توی کوئری های کاربران بین پر تکرار ها میگردیم .
