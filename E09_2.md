# E09_2

## مفهوم High and Low lists

دو تا لیست در نظر میگیریم ، های لیست میتونه همون champion لیست ما باشه و لو لیست هم میتونه شامل سند هایی باشه که وزن کمتری دارند توی posting list . بنابراین میتونیم پستینگ لیست رو به دو لیست تقسیم کنیم .

چجوری موقع جست و جو ازش استفاده میکنیم ؟ میایم بازیابی رو میزاریم روی های لیست ، اگر های لیست تعداد سند هایی که برام بازیابی کرده کمتر از top-k باشه اون هنگام باقی سند هایی که کم داریم رو از لو لیست میگیریم .

## مفهوم Impact-ordered postings

اون سندی که weight پایین تری دارند اونها تاثیر زیادی توی محاسبه نمیزارن و ماهم اونها رو دخالت نمیدیم و این سرعت رو افزایش میده . یعنی ما میتونیم پستینگ لیست هارو بر اساس $wf_{t,d}$ مرتب بکنیم و بعد بریم سراغ محاسبه ی امتیاز ها . حالا توی نرم افزار هایی که زمان براشون مهمه ، سند هایی که $wf_{t,d}$ پایینی دارن اصلا در نظر گرفته نمیشن .

مشکلش اینه که اون سازمان دهی یکسان دیگه وجود نداره ، یعنی چی ؟ یعنی ما میومدیم این posting list هارو تو حالت ابتدایی بر اساس doc-id مرتب میکردیم . و میتونستیم به صورت همروند محاسبه انجام بدیم . ولی وقتی بر اساس این $wf_{t,d}$ میاید posting list هاتون رو مرتب میکنید دیگه این common ordering وجود نداره و نمیتونید همروند محاسبه انجام بدید .

دو تا راه حل برای این مشکل وجود داره :

- مورد early termination
- مورد idf-ordered terms


## رویکرد early termination

میگه که posting list های مربوط به اون واژه هارو پیمایش میکنی در دو حالت بایست .
- شما ممکنه یک تعداد ثابتی رو در نظر بگیری مث r که بیشتر از r تا سند رو اصلا نیاز نیست پیمایش کنی

شما وقتی بر اساس wf پستینگ لیست رو مرتب میکنی یا r تای اول رو در نظر بگیر یا اینکه اگر wf از یک حد آستانه ایی کمتر شد دیگه پیمایش رو متوقف کن .

این کاررو انجام دادیم برای پستینگ لیست های واژه های توی متن جست و جو . برای نمونه ما سه تا واژه داریم و سه تا پستینگ لیست ، این سه تا رو پیمایش کردیم ، از لیست اول ۵ تا سند اومد ، از لیست دوم ۱۰ تا سند اومد و از سند سوم ۱۵ تا اومد . اجتماع این سند ها تشکیل یک مجموعه میدند . شما حالا این سند هایی رو که میخواین بازیابی کنید از این مجموعه استخراج میکنید . یعنی از پستینگ لیست های اونها مجموعه کوچکتری میسازید و از این مجموعه کوچک تر امتیاز هارو حساب میکنید و بعد رتبه بندی میکنید .

## رویکرد idf-ordered terms

از میان واژه هایی که بر اساس idf درون متن جست و جو مرتب میشوند ، اون واژه هایی که idf کمی دارند ، ما میتونیم اونهارو نادیده بگیریم . چون این واژه ها idfشون توی اون شاخص شباهت کسینوسی تاثیرگزاره .

چون شاخص tf-idf به دو ویژگی tf و idf بستگی داره ، و هرچقدر هم که tf بزرگ باشه باز چون در یک idf کوچ داره ضرب میشه و نتیجه یک عدد کوچک میشه و در نتیجه دیگه نمیتونه شاخص مناسبی باشه .

بنابراین ما فقط واژه هایی رو در نظر میگیریم که idf بالاتری دارند ، حالا ما میتونیم بیایم واژه های متن جست و جومون رو به صورت نزولی مرتب کنیم (از زیاد به کم) و بعد بیایم واژه هایی که idfشون خیلی اثرگزار نیست رو در نظر نگیریم و بیش از اون جلو نریم و در زمان میتونیم صرفه جویی کنیم .

## رویکرد Cluster pruning: preprocessing

ما تو همه ی این تکنیک هایی که تا الان بررسی کردیم هدفمون این بود که این شباهت کسینوسی رو برای همه این سند های موجود در مجموعه دادمون انجام ندیم یعنی هردفه که کاربر چیزی رو جست و جو میکنه ما نیایم شباهت این بردار متن جست و جو رو با تمام سند هایی که داریم حساب کنیم (بار محاسباتی بسیار بالا میره و همچنین زمان بسیار بالا میره و بهینگی برنامه به فنا میره) .

چگونگی : میایم خوشه بندی میکنیم سند هامون رو ، و سند هایی که شبیه هم هستند رو داخل یک خوشه میزاریم . و همچنین بیان کردیم که نمونه هایی که در یک شاخه قرار میگیرند بیشترین شباهت رو باهم دارند و بیشترین تفاوت رو با نمونه های موجود در کلاستر های دیگه دارند .
بنابراین زمانی که ما سند هامون رو (آنهایی که در پایگاه داده ی ما موجود هستند) خوشه بندی میکنیم این سند ها در خوشه های گوناگونی قرار میگیرند . یعنی بخش بندی انجام میشه براشون . اکنون برای هر خوشه یک مرکز (Centroid) یا (Leader) در نظر میگیریم که این مرکز درواقع نماینده اون کلاستر میشه ، اکنون اینکه چگونه محاسبه میشه داستان خاص خودش رو داره . برای نمونه میتونید یک بردار میانگین در نظر بگیرید و اون بردار میانگین میتونه به عنوان یک بردار مرکز در نظر گرفته بشه .
بنابراین یک مرکز خواهیم داشت ، هنگام جست و جو (که اون جست و جو خودش یک نقطه ایی در اون فضا هست) میان فاصله ی اون متن جست و جو رو از تک تک مرکز ها حساب میکنند و نزدیک ترین رو پیدا میکنند ، و به هرکدوم که نزدیک تر بود ما اون متن جست و جو رو عضوی از اون خوشه در نظر میگریم که مرکزش از همه ی مرکز های دیگه به متن جست و جوی ما نزدیک هست . و بعد میتونیم تمامی سند های در اون خوشه رو امتیاز دهی و رتبه بندی کنیم و به عنوان سند های انتخاب شده (نتیجه های جست و جو) به کاربر نمایش بدیم .
بنابراین محاسبه ی امتیاز هارو برای سند های موجود در اون خوشه که بچه های اون مرکز هستند لحاظ میکنیم.

### چگونگی این رویکرد

چگونگی و روش های بسیار زیادی وجود داره ، که این چیزی که بیان خواهیم کرد ساده ترین هست.

در این روش ما $\sqrt{N}$ تا سند رو به صورت کاتوره ایی (تصادفی) برمیداریم . و اونهارو به عنوان مرکز یا Leader در نظر میگیریم . حال برای هر سند دیگر (که مرکز نیستند) نزدیک ترین مرکز یا رهبر رو (که کمی پیش باهم پیدا کردیم) برمیداریم .

و این $N$ یک هایپر پارامتر هست ، یک چیزی که باید خودمون در نظر بگیریمش .

### مشکل اصلی

این حالت بسیار ساده و نخستین هست . و برای نمونه اگر دو مرکز کنار هم میفتادند اصلا محاسبات دیگه مناسب نبود .
بنابراین اون انتخاب مرکز ها به صورت کاتوره ایی ، ممکن هست که درون خوشه های یکتایی قرار بگیرند و پراکندگی خوبی رو نمایش ندند .

### چرا کاتوره ایی(تصادفی) ؟

چون سریع هست . ولی لزوما دقت مناسبی نداره .

## مورد Parametric and Zone indexes

هر سندی میتونه از بخش های گوناگونی ساخته شده باشه . برای نمونه یک مقاله دارای بخش های گوناگونی هست . و این سند دارای فراداده های (metadata) گوناگونی هست . اکنون هنگام بازیابی موتور جست و جوی ما باید توانایی داشته باشه تا روی این سند ها هم جست و جوی متن انجام دهد . برای نمونه وارد سایتی میشویم ، و در بخش جست و جوی متن چندین جا برای جست و جو میبینیم (Search Bars) یکی برای نویسنده ، یکی برای عنوان ، یکی برای انتزاع و دیگر موارد .

ما میتوانیم با به کاربردن این بخش ها پی آمد های جست و جوی خودمون رو بهبود ببخشیم . بنابراین میتوانیم این هارا به کار ببریم .

ما برای هر فیلد میتوانیم درواقع کلید ها (indexes) های گوناگونی داشته باشیم . ما برای این بخش ها inverted index خودشون رو میسازیم . و هنگام جست و جو میتوانیم کار های بیشتری انجام دهیم .

برای نمونه میتوانیم هنگام جست و جو بگوییم «واژه هایی رو پیدا کن که merchant در بخش عنوان قرار دارد و با متن جست و جوی gentle rain هماهنگ هست»








