# E02

## توکنیزه کردن

با استفاده از اسپیس بین واژه ها میتوان واژه هارا کد گزاری کرد و از متن بیرون کشید .

## پیش پردازش ها

پیش پردازش ها میتوانند در حل بعضی چالش ها به ما کمک کنند . چالش هایی که در توکنیزه کردن با آن مواجه میشویم بیشتر به مباحث و چالش های خود زبان بر میگردد . برای مثال در زبان انگلیسی با اسپیس میتوان این مشکل را حل کرد ، اما در زبان هایی مانند آلمانی ، فرانسه ، چینی ، ژاپنی و ... داستان بسیار متفاوت میشود و به راحتی نمیتوان چالش های آن زبان هارا حل کرد .

## حذف Stopword ها

واژه های Stopword واژه ها یا حرف هایی هستند که مفهموم خاصی رو بیان نمیکنند ، مانند ضمیر ها ، مانند واژه های ربطی که به مفهوم خاصی اشاره نمیکنند .

چرا اینهارا حذف میکنیم ؟ فرض کنید ما میخواستیم شاخص inverted-index رو بسازیم ، بردار نهایی این stopword ها بسیار بسیار طولانی میشد ، و بنا بر این برای نمونه برای واژه ی ٬the٬ یک برداری ایجاد میشد که شماره ی ایندکس تمامی سند ها در آن بردار بود ، و حتی هیچ کمکی هم به ما در بازیابی اطلاعات نمیکرد ، و حتی کار بازیابی اطلاعات ما رو بسیار کند میکرد .

اما با اینحا حذف این واژه ها چالشی دیگر را به همراه خواهد آورد ، برای نمونه جمله ی معروف شکسپیر را در نظر بگیرید "to be or not to be" ، این جمله سراسر واژه های stopword است و حذف خواهد شد . یا عبارت "king of denmark" ، اگر ما of رو حذف بکنیم این عبارت معنی خود را از دست خواهد داد .

> [!IMPORTANT] نکته
> الان دیگر به سمتی میرویم که stopword هارا حذف نمیکنیم (به گفته ی استاد) چون این واژه ها خود جزئی از زبان هستند .


## فرق term با token

یک سند رو وقتی به شما میدهند ، شما وقتی این رو توکن توکن میکنید ، در آخر این توکن ها همگی در یک جدولی به نام جدول عبارات ذخیره میشوند . یعنی آن چیزی که توی inverted-index اشاره میکند به posting list نامش term است . ***پس توکن لزوما ترم نیست*** . توکن تبدیل میشود به term . حالا یا بدون تغییر یا با یک سری تغییرات .

یکی از این تغییراتی که انجام میدهند normalization است .

## عمل Normalization

یعنی شکل های گوناگون یک واژه رو میتوانند نگاشت کنند به یک واژه . برای نمونه میگویند اگر واژه ی «U.S.A.» وجود داشت آن را به واژه ی «USA» تبدیل میکنند (نقطه هارا پاک میکنند) .

فایده ی این کار چیست ؟ واژه های بالا هردو به معنی آمریکا هستند ، ولی اگر نرمال سازی انجام ندهیم ، اولی یک بردار inverted-index خواهد داشت ، و دومی هم یک بردار دیگر . و حالا فرض کنید کاربر در حال جست و جوی متن است ، و به جای وارد کردن «U.S.A.» اشتباها واژه ی «USA» را وارد میکند ، ولی متن مورد نظر پیدا نخواهد شد چون نرمال سازی انجام نداده ایم .

بنا براین ، برای اینکه ما پستینگ لیست های گوناگونی برای عبارت ها و واژه هایی که ذاتا هم معنی هستند نداشته باشیم ، باید از Normalization استفاده کنیم .


> [!IMPORTANT] مشکلات این کار
> خود نرمالیزه کردن باعث بروز مشکلاتی میشود ، برای همین این کاررا انجام نمیدهند . و خطای به وجود آمده را گردن کاربر می اندازند .

### مشکلات این کار در زبان های دیگر

برای نمونه در زبان پارسی ما حروف صدا داریم (اَ اِ اُ) ، و اگر اینهارا نرمالیزه بکنیم ، آنگاه ممکن است که معنی و هدف جمله و عبارت دگرگون شود .


## عمل Case folding

در این عمل تمامی حروف بزرگ به حروف کوچک تبدیل میشوند .

گاهی اوقات این عمل باعث زیان میشود . برای نمونه «General Motors» یک نام ویژه و یکتا است . و این کار باعث میشود وقتی شما برای این نام یکتا جست و جو میکنید ، مستندات غیر مرتبطی که واژه های «motors» و «general» را درون خود دارند نیز در نتایج بیایند .


## الان دیگه کلا پیش پردازش مطرح نیست

اکنون دیگر گوگل از این ترفند ها استفاده نمیکند .

## عمل Thesaurus

یک خوشه از واژه های هم معنی میساختند ، مانند car و automobile ، و وقتی شما واژه ی ماشین را جست و جو میکردید تمامی مستنداتی که واژه های هم معنی car را داشتند هم در نتایج می آمدند . توجیح این کار این بود که واژه ها هم معنی بودند و بنا براین نتایج هم باید شامل تمامی آن واژه های هم معنی میشد .

## عمل query expansion

هر کاری که هنگام پیش پردازش روی مستندات خود انجام میدهید ، باید همان کار را روی عبارت جست و جوی کاربر هم انجام دهید . مانند کیس فولدینگ یا نرمالیزه کردن های دیگر .

یعنی اگر کاربر واژه ی «car» را وارد کرد باید واژه ی «automobile» را هم در نظر بگیرید . و این کار باید انجام شود .

## عمل Lemmatization

***شکل های گوناگون یک کلمه رو نگاشت میکنند به ریشه ی کلمه*** و ***این کاررا برای عبارت جست و جو هم انجام میدهیم*** .
برای نمونه am, is, are -> be یا car, cars, car's , cars' -> car . سود این کار چیست ؟ تعداد واژه های در فهرست واژه های ما کم میشود . هم سرعت بالا میرود هم فضای ذخیره سازی بیشتری خواهیم داشت .

از مشکلات این کار میتوان به این اشاره کرد که امکان بازیابی داده های بی ربط نسبت به عبارت جست و جو شده وجود دارد . ممکن است تعداد داده های بازیابی شده بالارود ، اما نتایج بی ربط در میان آنها بیشتر شده باشد .

## عمل stemming

شکل های گوناگون واژه به ریشه ی خود باز میگشت ، با این تفاوت که ممکن است ریشه معنی خاصی نداشته باشد .

برای نمونه automate, automatic, automation -> automat . و این واژه معنی ندارد .

## الگوریتم Porter's algorithm برای stemming

این الگوریتم هیچ ربطی به مهندسی کامپیوتر ندارد و توسط زبان شناسان حرفه ایی طراحی شده .
این الگوریتم ۵ فاز دارد . ما زیاد با جزئیات این الگوریتم کاری نداریم .

## خطا های Porter Stemmer

در نتیجه ای این کار FP شما بالا خواهد رفت . یعنی عبارت هایی که به عنوان نتیجه به شما نمایش داده خواهند شد اما در حقیقت هیچ ربطی به هدف شما ندارند .

## دیگر Stemmer ها

مانند Lovins که حدود ۲۵۰ قانون دارد .

## آیا Stemmer ها و دیگر روش های نرمال سازی کمک کننده هستند ؟

در زبان انگلیسی Recall افزایش می باید ، یعنی تعداد نتایج بالا میرود اما به precision یا دقت اطلاعات آسیب میرسد .
اما برای زبان های اسپانیایی و آلمانی و فنلاندی تا ۳۰ درصد بهینگی را بالا میبرند .


## مفهوم skip pointers

سر این باید ویدیو رو ببینی ، من نمیتونم این رو تو متن توضیح بدم . ولی درکل به جای یک دونه پریدن به جلو ، چند خونه جلو میپری و بعد دوباره مقایسه میکنی .

یک سری نقطه رو به عنوان اسکیپ پوینت در نظر میگیریم توی بردار ، هی حلقه رو با اینها جلو میبریم ، دیگه تک تک خونه هارو چک نمیکنیم .

باید یک تعادلی رو برقرار کنید توی اسکیپ پوینت ها ، اگر خیلی زیاد در نظر بگیرید ، آزار دهنده میشه ، و تقریبا باز اندازه ی حلقه ی معمولی انرژی میبره ، اما اگر کم در نظر بگیرید دیگه پرش اتفاق میفته .
این یک مقدار زیادیش مربوط به مشکلات هزینه های I/O هست که زمان زیادی از کامپیوتر برای پردازش میگیره و مشکل ساز میشه و دچار هزینه های زمانی سنگین باید بشیم .

یک معیاری رو به صورت هیوریستیک در نظر میگیرند . میگن که رادیکال طول بردار شما اندازه اسکیپ پوینتر هایی هست که باید بگذارید . که بدین صورته :

$$\sqrt L$$

> [!IMPORTANT]
> این فرمول برای موقعی هست که مجموعه مستندات شما ثابت هست نه اینکه به صورت پویا تغییر کنه.


## شبه کد skip pointer


```
IntersectWithSkips(p1, p2):

answer <-- <>
while p1 != NIL and p2 != NIL
do if docID(p1) == docID(p2)
	then Add(answer, docID(p2))
		p1 <-- next(p1)
		p2 <-- next(p2)
	else if docID(p1) < docID(p2)
		then if hasSkip(p1) and (docID(skip(p1)) <= docID(p2))
			then while hasSkip(p1) and (docID(skip(p1)) <= docID(p2))
				do p1 <-- skip(p1)
			else p1 <-- next(p1)
		else if hasSkip(p2) and (docID(skip(p2)) <= docID(p1))
			then while hasSkip(p2) and (docID(skip(p2)) <= docID(p1))
				do p2 <-- skip(p2)
			else p2 <-- next(p2)
return answer

```

